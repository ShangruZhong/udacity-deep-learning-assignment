{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业六是基于Text8语料训练一个LSTM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a\nHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIy\nMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCADuAmgDASIA\nAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQA\nAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3\nODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWm\np6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEA\nAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSEx\nBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElK\nU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3\nuLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1/wD4\nRCz/AOglrn/g3uP/AIukbwjZqpP9pa5wM/8AIXuP/i66Gmyf6tvoaAODt9HnubaOdLjUQsihgDrl\n1nB/GpP7Auf+fnUP/B7d/wCNa+lf8gmz/wCuK/yq5XMpy7nQ1FO1jnP7Auf+fnUP/B7d/wCNH9gX\nP/PzqH/g9u/8a0tT1m30yezt3jlmubyQxwQxAbmIGSeSAAB1JNTWF+t9aRTtBPatIWAhuV2SDBwe\nM+2fpRzS7i07GP8A2Bc/8/Oof+D27/xo/sC5/wCfnUP/AAe3f+NdEzBVLMQFAySe1NhmiuIllglS\nWNvuujBgfoRRzS7hp2Of/sC5/wCfnUP/AAe3f+NMn0WeC3kma41ErGpYga5dZwBn1rpqq6j/AMgu\n7/64v/6CaOeXcaUW9hsXhOzkiR/7R1wblBx/a9x/8XT/APhELP8A6CWuf+De4/8Ai6gh8eeFY4Y0\nfXLMMqgEF+hp/wDwn/hP/oPWf/fddJgyT/hELP8A6CWuf+De4/8Ai6P+EQs/+glrn/g3uP8A4uo/\n+E/8J/8AQes/++6P+E/8J/8AQes/++6BEn/CIWf/AEEtc/8ABvcf/F0f8IhZ/wDQS1z/AMG9x/8A\nF1H/AMJ/4T/6D1n/AN90f8J/4T/6D1n/AN90ASf8IhZ/9BLXP/Bvcf8AxdH/AAiFn/0Etc/8G9x/\n8XUf/Cf+E/8AoPWf/fdH/Cf+E/8AoPWf/fdAEn/CIWf/AEEtc/8ABvcf/F0f8IhZ/wDQS1z/AMG9\nx/8AF1H/AMJ/4T/6D1n/AN90f8J/4T/6D1n/AN90ASf8IhZ/9BLXP/Bvcf8AxdVdS0LR9I0+a/v9\nZ1qC2hXc7tq9xx/4/wAk9AB1rMvPi94UsdbWwlu2aBolcXkKl4wSSCpxyCMA9D1rJ8bazba9q+kW\n1ncpcafHA19mNsq7ltqZ+mHP1IqZy5Y3KjHmdjIutWvLpy2lx6pb2+fle/1q6Mjj12K/y/i2an0/\nWobeZU8QtrFvAxA+22mtXLxp/vqWyo9xkeuKz7q+WykDXChLYj/XZ4VvRvTPY/8A1qkt5HuYWaWD\ny0c/Kj9SvuO2fSuT6xPfodXsYbHpy+E7F0V01TW2VhkMNXuCCP8Avul/4RCz/wCglrn/AIN7j/4u\nuW8EeMNK0fRZtJ1fU4bd7G5aG3EzcmEhXX8t+36KK6X/AIT/AMJ/9B6z/wC+67E7q5yNWdiT/hEL\nP/oJa5/4N7j/AOLo/wCEQs/+glrn/g3uP/i6j/4T/wAJ/wDQes/++6P+E/8ACf8A0HrP/vumIk/4\nRCz/AOglrn/g3uP/AIuj/hELP/oJa5/4N7j/AOLqP/hP/Cf/AEHrP/vuj/hP/Cf/AEHrP/vugCT/\nAIRCz/6CWuf+De4/+Lo/4RCz/wCglrn/AIN7j/4uo/8AhP8Awn/0HrP/AL7o/wCE/wDCf/Qes/8A\nvugCT/hELP8A6CWuf+De4/8Ai6P+EQs/+glrn/g3uP8A4uo/+E/8J/8AQes/++6P+E/8J/8AQes/\n++6AJP8AhELP/oJa5/4N7j/4uj/hELP/AKCWuf8Ag3uP/i6zNZ+KHhfStMkvItQivWRlBgt2+dgW\nAJGeDgHP4VVvviTo2p+DdUvdB1BWvY4NqRupSSN2IVTg9cEg8ZHFAGRrl/YWt9Lp+iza3qFzC2ya\nZ9buEgibupYMSzDuAOO5FZEV7rED77kXV1F3jt9bu43x7FnIJ+uKSIWmlWcMDTRxRqNqmRwNx7nJ\n6k9TUFlq9rMZg97blhMyIPMXkZ4rLnbehm5Pc77QbHRPEdgbqy1TXwUbZNDLqtwskL/3WG/g/oe1\nav8AwiFn/wBBLXP/AAb3H/xdef6TqsPh3xdZX88629neK9rdsxwvCs6MfcFSM+jGu8/4T/wn/wBB\n6z/77rRO6LTuiT/hELP/AKCWuf8Ag3uP/i6P+EQs/wDoJa5/4N7j/wCLqP8A4T/wn/0HrP8A77o/\n4T/wn/0HrP8A77pjJP8AhELP/oJa5/4N7j/4uj/hELP/AKCWuf8Ag3uP/i6j/wCE/wDCf/Qes/8A\nvuj/AIT/AMJ/9B6z/wC+6AJP+EQs/wDoJa5/4N7j/wCLo/4RCz/6CWuf+De4/wDi6j/4T/wn/wBB\n6z/77o/4T/wn/wBB6z/77oAk/wCEQs/+glrn/g3uP/i6P+EQs/8AoJa5/wCDe4/+LqP/AIT/AMJ/\n9B6z/wC+6P8AhP8Awn/0HrP/AL7oAk/4RCz/AOglrn/g3uP/AIuj/hELP/oJa5/4N7j/AOLqP/hP\n/Cf/AEHrP/vuj/hP/Cf/AEHrP/vugCT/AIRCz/6CWuf+De4/+Lo/4RCz/wCglrn/AIN7j/4uo/8A\nhP8Awn/0HrP/AL7o/wCE/wDCf/Qes/8AvugCT/hELP8A6CWuf+De4/8Ai6P+EQs/+glrn/g3uP8A\n4uo/+E/8J/8AQes/++6P+E/8J/8AQes/++6AJP8AhELP/oJa5/4N7j/4uj/hELP/AKCWuf8Ag3uP\n/i6j/wCE/wDCf/Qes/8Avuj/AIT/AMJ/9B6z/wC+6AJP+EQs/wDoJa5/4N7j/wCLo/4RCz/6CWuf\n+De4/wDi6j/4T/wn/wBB6z/77o/4T/wn/wBB6z/77oAk/wCEQs/+glrn/g3uP/i6P+EQs/8AoJa5\n/wCDe4/+LqP/AIT/AMJ/9B6z/wC+6P8AhP8Awn/0HrP/AL7oAk/4RCz/AOglrn/g3uP/AIusVtEd\n7m4SC61QpFIY8ya3dAnAHoT602y+L3hS61afTprtrZ45jHHM6kxTDPBDDpn3xW1ZyJLNeyRuro1y\nxVlOQRhazqNpaGkEtbmP/YFz/wA/Oof+D27/AMaP7Auf+fnUP/B7d/410dFZc8u5enY5z+wLn/n5\n1D/we3f+NH9gXP8Az86h/wCD27/xrS1XWLfSfsqSpJLNdzCGCGIDc7Yz3IAAAJJJqWwvxe26yNbz\n2rs7IIblQr/KcEgZOR3BHajml3F7vYyP7Auf+fnUP/B7d/40f2Bc/wDPzqH/AIPbv/Gujoo55dx6\ndjnP7Auf+fnUP/B7d/41f0zw3bX1glxJf62jlnUqusXJA2sV/v8AtWpUmgf8ghP+usv/AKMarpyb\nlZkyS5b2KX/CIWf/AEEtc/8ABvcf/F0f8IhZ/wDQS1z/AMG9x/8AF10FFbGRz/8AwiFn/wBBLXP/\nAAb3H/xdH/CIWf8A0Etc/wDBvcf/ABddBRQBz/8AwiFn/wBBLXP/AAb3H/xdH/CIWf8A0Etc/wDB\nvcf/ABddBRQBz/8AwiFn/wBBLXP/AAb3H/xdFdBRQAU2T/Vt9DTqQjKkHuMUAc7pX/IJs/8Ariv8\nquUkfh6KKNY4769VFGFAkXgf980/+wl/6CF9/wB/F/8Aia51TkbuUW73MvVNFt9Ums7h5JYbmzkM\nkE8JG5CRgjkEEEdQRVZ9GuDrVrd/apHjht5Y/MeU7w7kHcFxtOMADjHtW7/YS/8AQQvv+/i//E0f\n2Ev/AEEL7/v4v/xNHJIV49zLtNOvLWYySa1fXo2kCK4WBVz65SJT+tGhWdzYaRFa3QiEqFs+U5Zc\nFiepA9fStT+wl/6CF9/38X/4mj+wl/6CF9/38X/4mnySC8e42quo/wDILu/+uL/+gmrn9hL/ANBC\n+/7+L/8AE01/D8UkbRvfXpVgQwMi8g/8BpezkNSine5pW/8Ax6w/7g/lUtNRQiKg6KMCnV0GDCii\nigAooooAKKKKACiiigDKuPDejXmsrq91p0FxfLGsSSzLv2KCSMA8A5Y8jmuR+JGnyW11Y+IUQtbw\nxta3hAzsQkFH+gbIP+9ntXodI6LIjI6hkYYZWGQR6GlKKkrMcZOLujw2azjvZFeZhLbhcrERlSfU\n+vtSAx6XZyNPcEwR8qX6qOy57+3fpXV+LfB2g+H9Gvdagvb/AEyCBdxgtnVoixOAFV1bbkkcDA9q\n3NE8B6JYzwai8txqlwuJIZruQMqZ6MiqAoPocZ965VhpXs3odLrq17aj/h/pFxpfh1pryJoru/na\n7kibrGCAqKfcKq59811VFFdSVlY5m7u4UUUUxBRRRQAUUUUAFFFFAFLVtIsdc097DUoBPauys8bE\ngMVYMM49wKoax4btLzwnfaHYQQWcc0JWJYowiI/VTgehArcooA8UtJ/tUbJcQ+VdQN5dxA4+aKQd\nQf6HuOaSztBB529E+eZnXA7HpXpuu+DtJ1+cXU6S296q7Rd2r+XJj0PZh7MDXIaD4S0fXbvU4X1z\nVbyPTrtrWSEyJEGIAzkxorYySOCOhrPk10IcCLwfZNrHi+K9jGbLSQ5aX+F52UqEHrtVmJ9CVr1O\nq9jY2mm2UVnY28dvbRDCRxrhRVirSsrFJWQUUUUxhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRR\nQAUUUUAFFFFAGVZ+G9GsNRuNRt9OgW+uJGlluCu6QsevzHkD2HFVLb/j6v8A/r6b+QroKy5NDiee\nWVbu7jMr72VHAGfy9qzqRbWhpBpXuMop39hL/wBBC+/7+L/8TR/YS/8AQQvv+/i//E1n7ORV49zM\n1XRrfV/srSvLFNazCaCaIgMjDjuCCCCQQRVYaPOuv2l81w88cMMiFpZSG3OQc7VAXooHatz+wl/6\nCF9/38X/AOJrH8PWsuqW989xf3YMGoXFsmxwPkSQqueOuBTUJIG4vqa1FO/sJf8AoIX3/fxf/iaP\n7CX/AKCF9/38X/4ml7OQXj3G1JoH/IIT/rrL/wCjGpv9hL/0EL7/AL+L/wDE1dsrOOwtEto2dlUk\n7nOSSSSc/iauEGndilJctkWKKKK1MgooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiii\ngAooooAKKKKACiiigAooooAKKKKAOG+LOjPrHge5P2x4IrQ/aGjVQfNI4AJ7DnP5V0+gaW+iaDZ6\nW92939ljESzOu0lR90EewwPwrN8f/wDIh6z/ANe5/mK6SgAooooAKKKKACiiigAooooAKKKKACii\nigBkqu0LrG+xypCvjO09jjvXnPwy8Jv4f1bxFOuqTXC/bGtpEkQDzGVVcSE56/Ow/GvSa5vwn/x9\neJf+wxJ/6KioA6SiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKK\nKACiiigArm/Bv/Hnq3/YYvf/AEc1b13b/a7Ke282WHzY2TzImKumRjKkdCK86+EWkazY22s3Gs3t\n1LIb6SBI5ZWZdysfMkAPUs3f296APS6KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooo\noAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDm/H/wDyIes/9e5/mK6Sub8f/wDIh6z/\nANe5/mK6SgAooooAKKKKACiiigAooooAKKKKACiiuc8e3k1j4G1aaBikjQ+UHHVd7BMj6bs0Ac1r\nHj++vrqW38OCCO0jYo2oTJv8wjr5S5AwDxuPB7DvWDp+reJNImuZ7XVorhrmY3E0V1bLskcgAnKY\nK8KOnp0qsYXgs1isxGhjUCNWHy4Hb/69VbLUm1GY+QoSKI7Zt/J3/wB0Y9PXp6e3C6027o7FSglZ\nnrHhbxVB4kt5UaE2uoW2BcWzNu256Mp/iU4OD7EGuhryDQJmtPHeiTRnBuGltJQP4kMbOPyZAa9f\nrqpz543OapHllYKKKK0ICiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApk\nsscELyyuqRopZ3Y4Cgckk0+uS+JMrx+B7uNCVW4lgt5GHZHlVW/MEj8aHoC1OX1LxtrGuSsdJnOm\naZn91KIw084/vfMCEU9hgnvx0rPttT8Q6c3mWWvXDnJYxXaJLG5Jyc4AIye4IqrdKwtmMUwgKDcG\nIG0Y9faqWnX02ozF2/cLEBmEj5nJ/i5H3fT17+lcDqzbumdvs4JWaPWvCfimPxHazJLCLbUbUhbm\n33bgM9HU91ODj6Edq6KvJfC0j2/xA0wx5/0m3nhlA7qAHGfoR/49XrVdlOfPFM5akeWVgoooqyAo\noooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACii\nigDm/H//ACIes/8AXuf5iukrm/H/APyIes/9e5/mK6SgAooooAKKKKACiiigAooooAKKKKACqOta\nXFrei3umTkrHdQtEWHVcjgj3B5/Cr1cd8RfE+q+F9Djn0ixiurm4kMS7myyHBORGOX4BPXjGTxQB\n55JHOkkujaqGt9QiG2VFOPNX++h7q3qOnTg09rW3hkS4GIfKTaSpCrs9D7D9K7zwv4ft9f8AAumy\n+JYE1K7u0+2SSzj5wZORtI5XC7RxjpVy3+HXhS3mWX+yvOKnKrczyTKP+AuxH6VyvDa6M6FX01Ry\nPgiGG/8AE9tqtxKkNtFG66eJDtN1Ifld0z95VU4467q9XrlfH/hNfFnhGfT4ERbyHEtmfuhZF6DP\nYEZH4+1UPBpuvCaQ+GvEFzNLdSsWtb6WZpIrn/YUt9xlH8HfqM846IxUVZGMpOTuzuaKKKokKKKK\nACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKztW13TNDiV9Qu0iMhxHGAWklPoiDLM\nfoKyPtHiXX+LWH+wbBv+W9yokunH+zH92P6tk/7NAF3xR4ig8OaRLcloXvNp+zWrvhrh/wC4oAJJ\nPsDXF6DrWt/FPRL+G60u30zRpUMazlmkkeQcgp0HysAc+ox647jSfDWm6PK1zDE899IMSXty5lnf\n6ueQPYYHtWnBBDbQrDBEkUS/dRFAA/AUAeHzK8FydJ1uFYL+MgmNzhJsdHQ/xKevt0NPuns7Yrd3\nTxxGMECRmxwe3v8ASvZtR0rT9Xt/s+o2Nvdw5yEnjDgH1GehrkvBPhvRIhqF2mk2f2mDU7qGKUwq\nWjRJCFCk9MCuZ4ZX0Z0Ku7aoreANCuXvpPEV9A8CtEYLKGVcPsJBaRgehbAAHXA969BoorojFRVk\nYSk5O7CiiimIKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA\nKKKKACimySJFG0kjqkaAszMcAAdSTVH+3tH/AOgrY/8AgQn+NAGhRWf/AG9o/wD0FbH/AMCE/wAa\nP7e0f/oK2P8A4EJ/jQBl+P8A/kQ9Z/69z/MV0lcd451fTLjwRq0MGo2ksrwEKiTqzE5HQA10P9va\nP/0FbH/wIT/GgDQorP8A7e0f/oK2P/gQn+NH9vaP/wBBWx/8CE/xoA0KKoLrmkuwVdUsmYnAAuEy\nT+dX6ACiiqUusaXBK0U2pWccinDI86gg+4zQBdorP/t7R/8AoK2P/gQn+NH9vaP/ANBWx/8AAhP8\naANCis/+3tH/AOgrY/8AgQn+NH9vaP8A9BWx/wDAhP8AGgC3cXENpbS3NxIsUMSF5HY4CqBkk1zn\nh+3m1nUD4ov42Tehj023kGDBAerkdnfgn0GB61n6lq2neJddGlyahapotiyyXbPMoF3L95Ihk8ov\nDN2JwPWuo/t3R/8AoK2P/gQn+NAGgAAMAYAorP8A7e0f/oK2P/gQn+NXIZ4bmFZoJUlib7rxsGU/\nQigCSqmp6ZZ6xYSWV/As1vIOVPBB7EHqCOoI5FW6jknhhx5sqR56bmAzQBzNpqd54au4tM16dp7K\nVgllqr9yekc3YP6N0b2PXqqpXb6ZfWktrdyWs1vKpSSORlKsD2IrmbfUX8I3Mdnc3f23QpGCW9zv\n3y2ZPASTuydg/UdG9aV0OzOzoqD7daf8/UH/AH8FH260/wCfqD/v4KLoLMnoqD7daf8AP1B/38FH\n260/5+oP+/gougsyeioPt1p/z9Qf9/BT454ZiRFLG+Ou1gcUXQWZJRRTJporeJpZ5UijX7zuwUD6\nk0xD6Kz/AO3tH/6Ctj/4EJ/jR/b2j/8AQVsf/AhP8aANCis/+3tH/wCgrY/+BCf40f29o/8A0FbH\n/wACE/xoA0KKz/7e0f8A6Ctj/wCBCf40f29o/wD0FbH/AMCE/wAaANCisW/8W6Bpto1zcatalF4x\nFIJGY9gFXJJ+grDj8Qaj4llWKxvbPRLSQ4V5ZY5byTP92PJVP+Bbj/sigDpdW13TNDiV9QukiaQ4\njiALSSn0RBlmP0FZH2jxLr/FrD/YNg3/AC2uFEl24/2Y+Vj+rZP+yK0dJ8NaZo8rXEMTzXsgxJeX\nLmWeT6ueQPYYHtWvQBkaT4a0zR5WuIYnmvZBiS9uXMs7/VzyB7DA9q16KKACiiigArm/Bv8Ax56t\n/wBhi9/9HNXSVzfg3/jz1b/sMXv/AKOagDpKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiii\ngAooooAKKKKACiiigAooooAKKKKACiiigAooooAwvGv/ACIniD/sG3H/AKLauYtdC0j7JD/xKrH/\nAFa/8u6en0rp/Gv/ACIniD/sG3H/AKLasi1/49If+ua/yrOZnU6FT+wtI/6BVj/4Dp/hR/YWkf8A\nQKsf/AdP8K0KKzMrs5/U08OaSY1udLtTJIrOqRWYdiq/eOAOgyPzq+uiaOyhhpVlgjI/0Zf8KyNW\ngLeN9HuPJvGiit5g7xCTYpO3aCR8ozg9fx7VtaRfPqWlw3jxeV5w3qmQflPK9/TFPpcbG/2FpH/Q\nKsf/AAHT/Cj+wtI/6BVj/wCA6f4VoUUhXZzXiPSdNt9GMsGn2kUi3FvtdIVUj98nQgV6pXm/in/k\nAv8A9d7f/wBHJXpFaw2NqewV5jY6Xp93quvy3NjbTSf2rMN8kKscYXuRXp1eeaT/AMhDX/8AsKzf\nyWiewT2JP7C0j/oFWP8A4Dp/hR/YWkf9Aqx/8B0/wrQorIxuzGv7HQNMsZby60yzWCIZYraBiOcd\nAM1YXRNHZQw0qxwRkf6Mv+FVfF32k+Fr/wCxfavtXl/ufsu/zN2eMbOasafqT3d/Pa+S6xwRRnzH\n4Jdhkgg8jAK9R3poetrj/wCwtI/6BVj/AOA6f4Uf2FpH/QKsf/AdP8K0KKQrsz/7C0j/AKBVj/4D\np/hWt8OkSPwRZxxqqIs1yFVRgAfaJOBUNT/D3/kS7X/rvdf+lElaQNKZ1FYerRRzazZrLGrqIJTh\nhkdUrcrG1L/kN2f/AFwl/wDQkp1PhOmn8RF9itP+fWH/AL9ij7Faf8+sP/fsVPRXOXdkH2K0/wCf\nWH/v2KPsVp/z6w/9+xUGs/b/AOxbz+y9n2/yW8jf0344ritIudZ0mxF/dS3tzO1vFC1vPHKimZnC\njPmNjdubBKkggdF4ppXByaO8+xWn/PrD/wB+xR9itP8An1h/79iplO5QfUZpaVg5mQfYrT/n1h/7\n9inaXFHDrlwsUaoptkOFXH8TVLTNP/5D1x/17J/6E1VD4kDb5WbVcv8AERVfwLqKOoZWMQIIyCPN\nSuormPiD/wAiPqH+9D/6NSukwMf+wtI/6BVj/wCA6f4Uf2FpH/QKsf8AwHT/AArQornOa7M/+wtI\n/wCgVY/+A6f4Uf2FpH/QKsf/AAHT/CtCuGnOu+XebRff2t/aQ+z48zyPI3DHT5Nu3Oc8598U1q7D\n1sdT/YWkf9Aqx/8AAdP8KP7C0j/oFWP/AIDp/hS6XqD6ilzI0JjSOd4kyQSwU7SeD/eDVfoFdmf/\nAGFpH/QKsf8AwHT/AArP1PS9Otb3Q5bewtYZBq1th44VUj5/UCugrK1r/j40T/sLWv8A6HTjuOLd\n0eiUUUVsdAUUUUAFFFFABXN+Df8Ajz1b/sMXv/o5q3ru5Wzsp7pkkkWGNpCka5YgDOAO59q4f4Z+\nKLDXo9YisUuDtv57ku8eFCyyMyDOeuO3tQB31FFFABRRRQAUUUUAFFFFABRRRQAUUUUAFISFUk9A\nM0tMl/1T/wC6aAMiPxPZSxrJHBfMjDKsLVyCPyp3/CSWn/Ptf/8AgK/+FZmjf8gWx/64J/IVerh+\nsTOuVOmm1b8SX/hJLT/n2v8A/wABX/wo/wCEktP+fa//APAV/wDCuf1TXTpev6XZSrCtrerLumd9\npQoufpg5Fa1vcwXcCz200c0LjKyRsGU/Qin7epa5PLT7fiWv+EktP+fa/wD/AAFf/Cj/AISS0/59\nr/8A8BX/AMKiopfWJhyU+34kv/CSWn/Ptf8A/gK/+FNfxNZRozvb3yqoyzG1cAD8qZVTVf8AkD3v\n/XvJ/wCgmj6xMcadNtK34nTI4kRXX7rDIp1RWv8Ax6Q/9c1/lUtdyOZ7hRRRQIKKKKACiiigDH8W\nW0174O1u1to2lnmsJ4441HLMUIAH1NchBqd8lvEjeG9cDKgB/wBGHp/vV6PRSaTE4p7nnn9q3n/Q\nt65/4DD/AOKo/tW8/wChb1z/AMBh/wDFV6HRU8iJ9mjzqTWLlI2eTw7rSxqCWLWy4A9/mpsN9PbR\nLFB4W1mKNeiJaKAPwDVa+L1/qFp4EuIdOt7iSS4YJLLChIhjByzEjpnAH4mup8OXt3qPh2wur+1m\ntr14QJ4pkKMsg4bg9iQSPYijkQezRyH9q3n/AELeuf8AgMP/AIqj+1bz/oW9c/8AAYf/ABVeh0Uc\niD2aPLtXl1HVLAWcPh7WEd54TuktwFAWRWJJ3egNeo0UVSVikktgrzWN9Q0zVtZR9B1WdZtQlmjk\nggDIynGCDkelelUUNXBpPc88/tW8/wChb1z/AMBh/wDFUf2ref8AQt65/wCAw/8Aiq9DoqeRE+zR\n55/at5/0Leuf+Aw/+KqGLUX82S4h8Mav5jkq8iWi5YjjBO7tjH4V6Fe3Js7C4uVhknaKNnEUS7nc\ngcKB6npXnnwg1TWrvT9Wt9ZtLmORb6WVZZIyFLMx8xAfVXB4/wBr2o5EHs0T/wBq3n/Qt65/4DD/\nAOKo/tW8/wChb1z/AMBh/wDFV6HRRyIPZo88/tW8/wChb1z/AMBh/wDFV0HgS2ubTwfaRXltLbTm\nSd2hlGGUNM7DI+hFdHRVKKQ1FLYKxtWWZNStZ0tppkWKRW8pc4JK4/ka2aKJLmVjSMuV3Of+1Tf9\nA69/79j/ABo+1Tf9A69/79j/ABroKKz9ku5XtF2Of+1Tf9A69/79j/GoZ5UuR9muNLuJQfn8uSFW\nzg9cE9jiumry6TXNbHxzjjGm339li1+yE+S2CmcmX/d8zA3elHsl3H7RdjsBcygYGm3oA/6ZD/Gl\n+1Tf9A69/wC/Y/xroKKPZLuL2i7HP/apv+gde/8Afsf41NpazPqs87200KGBEHmrjJDMf61tUU1T\nSdwdTS1grnvHNrc3ng2/gtLeS4nPllYoxlmxIpOPwBroaK0Mzzz+1bz/AKFvXP8AwGH/AMVR/at5\n/wBC3rn/AIDD/wCKr0Oio5ER7NHnn9q3n/Qt65/4DD/4qmvrN1Gu5/DutqCQuTbKOScAfe7k16LX\nm/xh1LWLTQ9Pt9Htbl5JLyKR5ooyVQq48tSfVpCmB7UciD2cRYb2a3DCHwtrMYZizbLRRknqT83W\npP7VvP8AoW9c/wDAYf8AxVd1p1097pttdSW8tvJLGrvDKpVoyRypB7g1Zo5EHs0eef2ref8AQt65\n/wCAw/8AiqrXEmo6nqGkRR6Bq0Ij1GCaSSaAKiIrZJJ3V6ZRTUEgUEncKKKKosKyZfENnFcSw+Vd\nyNE+xzHbswB+orWrmLX/AI/NS/6/G/ktY1puCTRrSjGV3Iv/APCSWn/Ptf8A/gK/+FH/AAklp/z7\nX/8A4Cv/AIVFRXP9YmaclPt+JL/wkdp/z7X/AP4Cv/hXPeEYNL8J2V9b21rfYuryW44tH+VSfkXp\n2UD9adq+ttpWraTbusItr2R45JXfaY9qFs+mOK1ba6t7yAT2s8c8TZAeNwynHB5FP29S1w5afb8S\n1/wklp/z7X//AICv/hR/wklp/wA+1/8A+Ar/AOFRUUvrEw5Kfb8SX/hJLT/n2v8A/wABX/wrQsb2\nHULNLqDd5bkgb1KnIJByD7g1lVN4a/5Acf8A12m/9GvWtGrKcrMmcIqF1/W5r0UUV0mAUUUUAFFF\nFABRRRQAUyX/AFT/AO6afSONyMvqMUAjldG/5Atj/wBcE/kKvVVtNI1q0s4bdW09hEgQEs/OB9Km\n+wa3/wBQ/wD76f8AwrzlSn2OyXK5NpoxNY0i9vfEui6jAts0FgZS4lkZWJddowApHHXrWboun6zo\niaXp0pjEf2q4nma3DyKysSVUkhdvL56fwfhXW/YNb/6h/wD30/8AhR9g1v8A6h//AH0/+FUoVLWs\nS0n1RRhn1trsLPp+npbbuZEvnZwPXaYQM+26o9DeZhfrKLnC3b+X9oVx8nGMFuo69K0vsGt/9Q//\nAL6f/Cj7Brf/AFD/APvp/wDCl7OfYGl3RJVTVf8AkD3v/XvJ/wCgmp/sGt/9Q/8A76f/AAqK50nW\nrm1mgLaeolRkJDPxkY9KXsp9io8qkndDYPA3h+S3jdrW53MgJ/06cc4/36k/4QPw9/z63P8A4Hz/\nAPxddDChjgjjJyVUA49hT69FbHI9zm/+ED8Pf8+tz/4Hz/8AxdH/AAgfh7/n1uf/AAPn/wDi66Si\ngRxlho9nonxEtoLATxxTaTO7o9xJICwlhAOHY4OCenrXZ1zc/wDyUux/7A9z/wCjoK6SgAooooAK\nKKKACiignAyelAHN+P8A/kQ9Z/69z/MV0lee+M/F+lanoGpaRpZuNQuJYzFvtYS8Stnu/wB09OxN\ndBpPjbRtXu1s1kmtL1/uW95EYnf/AHc8N+BNK6vYrlla9tDoqKKKZIUUUUAFFFFABRRRQAVzfgv/\nAJB2p/8AYYv/AP0oerGteLtH0KcW11O8l4w3C1tozLLj1Kr0HucCuV8MeM9M0uC7h1SK9sFn1C5u\nEmntyIwskrOu5hkKcEZzge9K6vYpRk1dI9GopsciTRLLE6vG4DKynIYHoQadTJCiiigAooooAKKK\nKACubb/kpcf/AGB2/wDRy1rarrGn6JZm71K7jtoc7QXPLH0UDlj7DmuFPjSyPjJdWGn6qbIWBtvM\n+xtncZA2dv3sYHpn2pNpblKMpbI9IoqhpOs6drlp9q027juIgdrFeCp9GU8qfYgVfpkhRRRQAUUU\nUAFFFFABXN+OP+QBB/2ErH/0pjrondI42kkZURRlmY4AHqa888VeM9L1XT0tNKjvL8x3lvM01vAT\nFiOZHbDnAY4U4xkZpNpbjUXJ2R6LRWFo3i/R9cuDa288kN4BuNrcxmKTHqA33h7jNbtMGmtGFFFF\nAgooooAK5i1/4/NS/wCvxv5LXT1zzaTqsV5dyW7WZimmMo8wsGGQBjge1YV4uSVjai1qmySio/sG\nt/8AUP8A++n/AMKPsGt/9Q//AL6f/CuX2U+xrp3Ri67pF5qOs6JdW62xhsLhppRLIVJypUbQFPIz\nnkjpWZpun61o32a1cxKlxqk1zK1uHlHltuYKThdvzEe1db9g1v8A6h//AH0/+FH2DW/+of8A99P/\nAIVShUWlhNRfVf1/w5JRUf2DW/8AqH/99P8A4UfYNb/6h/8A30/+FT7KfYendElTeGv+QHH/ANdp\nv/Rr1V+wa3/1D/8Avp/8K0tGsptP0uO2nZGlDOzGPO35nLcZ+tbUISjK7RNRrktfr/mX6KKK6zmC\niiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDm5/+Sl2P/YHuf8A0dBXSV5d\nqOseJIvjhY6bFbWJge1ZY5Wjf/j2Yq7k/N94GLaD056c16jQAUUUUAFFFFABXC+P7+W5ubPw7DIy\nQ3MbXF6VOC0KkAR59GY8+ykd67qvPPG8LWni7Tb9xiC7tWs9/ZZFbeo/EF8f7tZ1W1BtG2HUXVip\nbGRNc2Wl2qmaWG1gX5V3EKo9hVffpuv2bxxzJcRgg7425RuoYHsR1BFRzwXKaxNdi3M4+zrHAAwA\nRsndnJ4z8vPtVnTLM6dpdvau4YxJhmHAz3/CvO0Sv1Pb1btbQ7jwPq9xqugsl6/mXtjO1pPJjHmF\ncFX+pVlJ9ya6WuM+HELNpGoamQRHqF60sP8AtRqqxq347CfoRXZ16cb8queBNJSaWwUUUVRIUUUU\nAFY3ivWJNC8NXl/AqvcgLHbq3QyuwRM+25hn2zWzXMfEC0lufCFxLAjSSWksV2EUZLCN1dh9doNJ\n7aDja+pxNnaR6dbyPLKZJnJkubmU/NK/dmP+cUy31nS72f7Nb3tvNIcjYjBs1HqMZ1G1s5IMTWxm\nSWRVI/eR4JGM9Rnafwp9hayrfXl7Mnlm4KKseQSFUHBOOMkk/hivL3u5bn0GqajFaG54Iu30nX5N\nBDH+z7qJ7m0QniF1I8xF9FIYMB2Ib1r0WvM/DcLX3j23aMZj021kkmYdA0mFRfqQHP4CvTK9Ci24\nJs8bFKKqtRCiiitTnCiiigAooooA8dl1WHXNek1a8mQl2lTT4nbiKBDtLAerfeJ9CB0FW/tdsSoF\nxES33QHHzc44/Hisez0hfs0mlzySw3OnGayk2EA7SeG5HdQrA+9TL4ct124urn5WLDlOpdXP8Pqo\n/WvNqWc3zM92iuWmuRElvq8el3kHiWxYxhGVL+LI/eQFsHcAcbl+8D7EdDXsteG3ujrHZPp1tLNL\nc6nssokfb3ZsngDoGYk+gr3EDaoA6AYrrw793yPOxqSmu/UWiiitzjCiiigAooooA888c3j6prcP\nh/cfsMEK3V4oPEpZiI4z/s/KzEd/lrHur+x02JTdXMNunRd7Bc49K0fFETWPjx5JBiLUrNPKY9DJ\nEWDL9drKfz9K56e0u/tepOsHmyXKLHA7EbUXbgg9/vEk4HPFcFe7qWZ7GDSVFOO73LLiy1yzEltc\nAtG26G5hPzQyDkMp7EcV6P4R1iXXPDdtd3IVbtS0NyFHHmoxViPYkZHsa4C3jj07TYonk/d28QUu\nxxwo6n8q7D4eW0sPhOO4mRka+nluwjDBCO5Kf+O7T+NaYZ6tLYwxyXLGT3OqooorrPOCiiigAooo\noAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiig\nArB8ReK7Pw95UBjku7+cEw2kGN7D+8xPCr7n8M1vV4ZH4m0q51LUdSvdStku7q5cbXkAMcasVjT2\nwoB+pNaUoKcrN2M6k3GN0jbl8Za0/iq2uP7D0pboWcuwG5ckRl48qX29c47Y612nh3xha65O1jPb\nyWGpou82srBt6/3kYcOPyI7gV5M/iDRz4ot5xqVr5S2UiF/MGAxdCBn14P5VLqniXSo4I7+y1S1N\n/YuLi32yjJZeq/RhlSO+a3nRhZ8rMY1Z3Skj3iisH/hNvDOBu1yxQkA7WmUEUf8ACbeF/wDoP6d/\n3/WudU5vZM6bo3qKwf8AhNvC/wD0H9O/7/rR/wAJt4X/AOg/p3/f9aPZT7MLo3qpatpNnremy2F/\nF5kEg5wcFSOQynsQeQazv+E28L/9B/Tv+/60f8Jt4X/6D+nf9/1o9lP+VjujjNY8Pa/4bsbi7S6s\n9Ssbdd26dmhnC++1WVj7/L9Kv2vgTVNScDX7u3hsv47OxZmMo/uvIwBA9QoGfWrPi/xb4evPCepW\n1trNlNPJFtSOOYFmORwBXc1Dw0YJScbP/hjZ4mq1y82gyKKOCFIYY1jijUKiKMBQOAAPSn0UUGIU\nUUUAFFFFABRRRQBwWoeA7u0uJJvDd1bxwSMWNhdgiJCevluuSg/2cEemKzNP8P8AibWBJ+803T4U\nleF5Vd7hwysVbapVR1HUn8K9QrH8Of8AHpe/9hC6/wDRrVk6UHK7Rf1utCSgnpr+hJoOg2Xh7T/s\nlmHYsxkmmlO6SZz1Zj3P6DoK1KKK1I3CiiigAooooAKKKKAOZ8R+EI9YuV1GxufsOqIuwzbN6TKO\niyLxkDsQQRn8K5T+xPFY1Ead5GjmYxeb532qTbtzj7vl5zntn8a9RrIP/I4r/wBg9v8A0YKSowqN\nuS6FfWatJJQe7M7w54OTSbv+0tQuvt2qFSiybNkcCnqsa5OM9ySSfYcV1FFFCSSshSk5O7CiiimI\nKKKKACiiigDN1zQrLxBpxs71XADB45YzteJx0dT2I/8ArHiuB1Lw/wCJNEiD+bp2o25kSJZXd7eT\nLsFXcoVh1I5BH0r1CsXxT/yCIv8Ar9tf/R6VnUhGUbtHThJyVWMU9G0c3YeAr29mSTxJdW72yMG+\nwWgYxyEdPMdsFh/sgAeua74AAYAwBRRVRioqyMZzlN3k7hRRRVEBRRRQAUUUUAFFFFABRRRQAUUU\nUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAV5AbQ6HrV5ok67d\nsrz2jEcSwuxYY9SpJU/Qetev1m61oOm+ILVbfUbfzAjbo5FYq8TeqsOVP0rSlUdOVzOpTU42PJZA\nP+EutuB/x4S/+jI6nv7ZtXli0C1Gbm/+R9o/1UP/AC0kPoAuQPUkCtOfwZp8XjW00n/hJdUF1LYy\nyoCIS4jDr8u7Z1JBOcZ+Q13eh+GtM8PRSLYwt5suDNcSuXllI6bmPP4dB2FbzxKaaitzGOHaacns\ncdqOkv4V8R+fYyT2+n3pLL5Az5TjkrsPDDHIXrgHBBHPUQTa1NAksNzos0bqCsgVxuHrjNaup6fF\nqlhJaSkruwUdfvRsOVYe4ODXMR6XKEAu/BthcXA4kmR4lWQ/3gCMjPXHvXInBaSv8jqqYZ1n7Sm0\nm905W+fz6+evU1s6/wD39G/KT/GjOv8A9/Rvyk/xrM/sxP8AoRrT/v7DR/Zif9CNaf8Af2Gnel3f\n3EfUsR3X/gaNPOv/AN/Rvyk/xozr/wDf0b8pP8azP7MT/oRrT/v7DR/Zif8AQjWn/f2Gi9Lu/uD6\nliO6/wDA0aedf/v6N+Un+NL5niH/AJ66P/5E/wAay/7MT/oRrT/v7DR/Zif9CNaf9/YaL0u7+4Pq\nWI7r/wADRqeZ4h/566P/AORP8aPM8Q/89dH/APIn+NZf9mJ/0I1p/wB/YaP7MT/oRrT/AL+w0Xpd\n39wfUsR3X/gaNTzPEP8Az10f/wAif40eZ4h/566P/wCRP8ay/wCzE/6Ea0/7+w0f2Yn/AEI1p/39\nhovS7v7g+pYjuv8AwNGp5niH/nro/wD5E/xo8zxD/wA9dH/8if41l/2Yn/QjWn/f2Gj+zE/6Ea0/\n7+w0Xpd39wfUsR3X/gaNTzPEP/PXR/8AyJ/jR5niH/nro/8A5E/xrL/sxP8AoRrT/v7DR/Zif9CN\naf8Af2Gi9Lu/uD6liO6/8DRqeZ4h/wCeuj/+RP8AGrGi20lhZSR3M0LzSTyzMYz8oLuWwM/WsP8A\nsxP+hGtP+/sNH9mJ/wBCNaf9/YaL0u7+4I4Oupc3uv8A7fR1vmR/31/OjzI/76/nXJf2Yn/QjWn/\nAH9ho/sxP+hGtP8Av7DT5qXd/ca/V8R/d/8AA0db5kf99fzo8yP++v51yX9mJ/0I1p/39ho/sxP+\nhGtP+/sNHNS7v7g+r4j+7/4GjrfMj/vr+dHmR/31/OuS/sxP+hGtP+/sNH9mJ/0I1p/39ho5qXd/\ncH1fEf3f/A0db5kf99fzo8yP++v51yX9mJ/0I1p/39ho/sxP+hGtP+/sNHNS7v7g+r4j+7/4Gjrf\nMj/vr+dZG5W8YrtYH/iXnof+mgrJ/sxP+hGtP+/sNWLNLnT5GksvCENu7DazRXESkj04pqpTina/\n3EvC15NX5d/5kdPRWL/aetf9C83/AIGR0f2nrX/QvN/4GR1lzr+kzf6tPuv/AAKP+ZtUVi/2nrX/\nAELzf+BkdH9p61/0Lzf+BkdHOv6TD6tPuv8AwKP+ZtUVi/2nrX/QvN/4GR0f2nrX/QvN/wCBkdHO\nv6TD6tPuv/Ao/wCZtUVi/wBp61/0Lzf+BkdH9p61/wBC83/gZHRzr+kw+rT7r/wKP+ZtVi+Kf+QR\nF/1+2v8A6PSj+09a/wCheb/wMjqnqTa1qsENsdFMC/aYZGka6RtqpIrHgdeBUyknFpfkbUKMoVYy\nk1ZNfaj/AJnTUUUVocQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFF\nABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB5lf8AhO6n+NNlqw1l0lFq9yieQCFSNkjMX3uj\nCRsn1Nem1zc//JS7H/sD3P8A6OgrpKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKK\nKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooo\noAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApr/AOrb6GnU2T/Vt9DQB5aL+9IH+mXP\n/f1v8aX7def8/lz/AN/W/wAarL90fSlr1lGPY4Lssfbrz/n8uf8Av63+NH268/5/Ln/v63+NV65v\nXNTuLG9zb35MQT/SY1RGNspIAk6Z/A5657VMuWO6HG7Os+3Xn/P5c/8Af1v8aPt15/z+XP8A39b/\nABrJtL+Ke4+zRSNPsiWQz4GGySByBjPB6VequWPYV2WPt15/z+XP/f1v8aQ397g/6Zc/9/W/xqCk\nPQ0csewXZ6xASbeMk5JQfyqSuYin8Z+SmzTtAKbRtLX8wOP+/NP8/wAbf9A3w/8A+DCb/wCM15J3\nnSUVzfn+Nv8AoG+H/wDwYTf/ABmjz/G3/QN8P/8Agwm/+M0AE/8AyUux/wCwPc/+joK6SuY07TvE\nE3i2PV9Xi0yGGKxktkS0uJJGLPJG2TuRcDCGunoAKKKKACiiigAooooAKKKKACiiigAooooAKKKK\nACiiigAooooAKKKKACiiopp1hHPLHoKAJaKZHIsq7lP/ANan0AFFFFABRRRQAUUUUAFFFFABRRRQ\nAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAB\nTZP9W30NOooA8iB+UUuRXrlFdf1t9jn9h5nkeRRkV65RR9bfYPYeZ48tvCly9wqkSuAGOTzj26VL\nkV65RR9a8g9h5nkeRQSMGvXKKPrb7B7DzI7f/j2i/wBwfyqSiiuQ6AooooAKKKKACiiigAooooAK\nKKKACiiigAooooAKKKKACiiigAooooAKKKKACiio55fKj3YyegoASecQr6segrNZi7FmOSaljRrm\nU7m9yavJBHGOFBPqaAM6ORo23Ka0IZ1mXjhh1FOaKNxhkFU5oDARIjcZ49RQBfoqG3n85TkYYdam\noAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiig\nAooooAKKKKACiiigAooooA//2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(\"LSTM-1.jpg\"))\n",
    "display(Image(\"LSTM-2.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                            saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299094 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "fo iow s rlzwnp rcma i fn syndrenw vnfatemn a  fwahg cfsuy isqoufyectsntleoiwqfn\n",
      "tajfyerfasopgktgqa gyl b  yihvuitxlinrrv o hg im acietfzuebc dndoaf peqshr letta\n",
      " sfk dvssagm ecrilzdtfe aytx paohjf lqkniqpmlslpflitanajz lrve eic tiy ein ekykg\n",
      "hslvtbautal  me zzi pcr t csoygiti   ouenn magaxdg airzpuetppuircw  u e l wamykk\n",
      "f sl  umh vycl pecjfdo yjt   opei xqqhuoszlxtconzuler  cfonb inu phaue hgyl shvk\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.599800 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.03\n",
      "Validation set perplexity: 11.54\n",
      "Average loss at step 200: 2.241100 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.17\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 300: 2.103457 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 400: 2.002189 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 500: 1.942987 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 600: 1.915353 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.865927 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800: 1.822325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.831198 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1000: 1.826320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "wious two zero greaded the ar one complation mod sudfore of live from sivol comp\n",
      "y hix appocud the modume becove mirakions on acceores the helbogle to s utecled \n",
      "le cefter shwilg deap was mesic histerration of mooturitaly statioule to in erge\n",
      "zer us contrreding in besustles peroce of the sight fur sopporrect asa lach becr\n",
      "matural resisteral fren futle windut had to sictioner over s of refremente the l\n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1100: 1.777408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1200: 1.757358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1300: 1.732955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1400: 1.753016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1500: 1.740464 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.750099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1700: 1.714525 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1800: 1.676468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 1900: 1.646691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2000: 1.697720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "================================================================================\n",
      "cian in the mass popules chode works and and that we perroal deading conlatic do\n",
      "ner to ressunged day emperors undrectorni cross acion to ady had s madeire can c\n",
      "w wak is the rombernived as who mir on carress in x north were spangral did clak\n",
      "d ying costivu absvimel he dygine termus preforting joz d opered precurds in two\n",
      "n colrent pashity popul basover to alegionorling af volin its form themar astica\n",
      "================================================================================\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2100: 1.685452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2200: 1.679117 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2300: 1.638991 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2400: 1.658381 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2500: 1.674526 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2600: 1.648821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2700: 1.650480 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2800: 1.645783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2900: 1.644826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3000: 1.646790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "ry is lists whall operation hallamazing repating first becalled one two five gam\n",
      "y he havagag inflard pood demical be undermine kniak in afterab stanel kith ptod\n",
      "n indivedabas counties and of entinie in zargerscup for the texe was plays mala \n",
      "x resceined and others iba experss counality stengawin a maili hudor felrea in w\n",
      "part his society two to permnists alsosya yearshed in inficent rey rases that th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3100: 1.624074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3200: 1.640818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3300: 1.633958 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3400: 1.661946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3500: 1.651658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3600: 1.663823 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.640745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3800: 1.641510 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3900: 1.633345 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4000: 1.652491 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "dedy of coame two zero zero zero sco eg five five one two one numensisted behous\n",
      "prich at sough heirks and designedomiry in geneals in the one nive nine when has\n",
      "ba found eestsum till fredeett semen pagrantur nupuares a delakers conficiation \n",
      "mer laser the becames s compractrese de one zero zero zero one nine one zero two\n",
      "kis other henders as chowest and designes dued kils for canimate who seasone his\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.628096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.634247 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.612246 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4400: 1.606063 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4500: 1.611106 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4600: 1.611542 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4700: 1.621619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4800: 1.627698 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4900: 1.632013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000: 1.606962 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "ally since end other parchal kebreine six four two valical one zero zero one nin\n",
      "rear a aremett rehome primate mutames had backethrop by this oth with with e and\n",
      "man ormed of autilaty that a bocimur for gwallly to actorms s siborment of such \n",
      "han south commonled five five one nine two specher and this be llastranceas hono\n",
      " to hour often fives tiblere consouncted of gerry used a one nine eight s vine t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5100: 1.605732 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5200: 1.588887 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5300: 1.581473 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400: 1.577290 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5500: 1.569677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5600: 1.580150 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.573071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.584508 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5900: 1.575491 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6000: 1.543791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "jures chilty had aramace b two four and genuletians that appli aurolity the dema\n",
      "ster sumical takit slansuge the crata disof the it usians soria for can rosh the\n",
      "f and list is two zero bight b schistlands the and toward boyshes kain of that t\n",
      "oldy for the conversive united it four three sou petrupt rome framy ment mic s i\n",
      "ing is see and zerogen films the comminoters chakishon other latterment sow bact\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.566289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6200: 1.535690 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6300: 1.547359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.537675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.556414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6600: 1.597477 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6700: 1.575082 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.601930 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6900: 1.585406 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 7000: 1.577820 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "x early his cluscred noted the public rebyon philamed adong at that things solve\n",
      "bise one one eight kranno run off promative rai injection withish sak of shaddea\n",
      "ulticous tox degraphie people and like go sonksoocn with beginstanms newn howtho\n",
      "a dans m hilo of empire viewworld in earth were bijther church at s controps mil\n",
      "chiratirs on the one nine two two six exatelipits been al one nine zero no on al\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(1, 27) * (27, 256) + (1, 64) * (64, 256) + (1, 256)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias. \n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # 27*64\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # 27*64\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # 27*64\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # 27*64\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "    \n",
    "    # 改动，拼接同类的tensor\n",
    "    tmp_x = tf.concat([ix, fx, cx, ox], 1) # 27*(64+64+64+64) = 27*256\n",
    "    tmp_m = tf.concat([im, fm, cm, om], 1) # 64*(64+64+64+64) = 64*256\n",
    "    tmp_b = tf.concat([ib, fb, cb, ob], 1) # 1*(64+64+64+64) = 1*256\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "#   # Definition of the cell computation.\n",
    "#     def lstm_cell(i, o, state):\n",
    "#         \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "#         Note that in this formulation, we omit the various connections between the\n",
    "#         previous state and the gates.\"\"\"\n",
    "#         input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#         forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#         update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#         state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#         output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "#         return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        print(i.shape, '*', tmp_x.shape, '+', o.shape, '*', tmp_m.shape, '+', tmp_b.shape)\n",
    "        smatmul = tf.matmul(i, tmp_x) + tf.matmul(o, tmp_m) + tmp_b \n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul, 4, 1)\n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                            saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293980 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "s gdacl qrqovfnjbflkvnmd po  ssv qhofwtnlea mesyxepqhn  t grxf htw br on ubcmr i\n",
      "qscgezbufmb o  katndhnemsscborxn  ptihbcsvgkbgeaex  ichgwaekdoimzjwinnms tm  osd\n",
      "s i cc jht nvvn i  hrhh igwtjlmhzek  d ou q ejyftjre okpefidudy wiqarreyxeagpyhl\n",
      "dwz tkca irmdhiiyxge kt  ta  kxrqn irikebceti fin imrgnixio heod  a nhtmymcypenw\n",
      "tyqxt mj s zoquerzikssvobsi o   zcrrdeh ok k ebdenroa kjltttzcezapdrefv tdpv srv\n",
      "================================================================================\n",
      "Validation set perplexity: 20.00\n",
      "Average loss at step 100: 2.589450 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.68\n",
      "Validation set perplexity: 10.62\n",
      "Average loss at step 200: 2.244830 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.33\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 300: 2.082478 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 400: 2.026444 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 500: 1.972901 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 600: 1.891720 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 700: 1.862598 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 800: 1.862666 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 900: 1.840401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 1000: 1.843866 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "revil ca rachorntocal infide altusts amerab one one serat with delo as base ogjo\n",
      "f bly pth he a nending ougige the ray rebally in the lactism in a lart bus hyst \n",
      "grent to but ial one jign thrye a scielly who nation we kai d other two grop of \n",
      "val autism and int in hissonia storelly outs une licters of compat has g uspon r\n",
      "thort one seven three feance niger tex supfive the has mosere elfored appomical \n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.799014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1200: 1.772848 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1300: 1.755795 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1400: 1.765081 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1500: 1.748151 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1600: 1.729649 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1700: 1.716323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.690886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.694858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2000: 1.678362 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "por tims in ofownctom absief in office stars fronces the texmly this the tendent\n",
      "x the centrations for the moded the alsoman deat elforence contases the relipes \n",
      "s ethee to britiovardy ite foll one zero zero zero zero zero zero the ire to as \n",
      "um throenical frica most that them and one five offonderly well re sixzald spati\n",
      "g porptem delicaties fouct systems the icantimbar his aft syrrocic althore yarli\n",
      "================================================================================\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2100: 1.689694 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2200: 1.704462 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2300: 1.709979 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2400: 1.684139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2500: 1.690292 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2600: 1.673041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2700: 1.681663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2800: 1.683016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2900: 1.672283 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3000: 1.684067 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "pay anothiric c i vost the mari con ampits external after of two form of wit use\n",
      "tem rutth be as wht welj in or id one seven two eight eight four eight zero five\n",
      "fraliatority amerimage tund opes tew irpa a purna chauss rated britozies is a le\n",
      "jer one zero of the east are a legity to after forushation the one eight one two\n",
      "jughting for traxition of zero zero and in terms to thk shate netatory refeal as\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3100: 1.648060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3200: 1.631907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.644840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3400: 1.636244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3500: 1.672528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3600: 1.651639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3700: 1.646605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3800: 1.655302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3900: 1.650736 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4000: 1.638434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "zient acchssons singers and cubtors in the epilike the resomenimatic internienty\n",
      "hish longed shorth shive weep bulibatasises to the spinist of a whand kalleasch \n",
      "t suffedeen claked vannetos coppiakipalefeerd ghoup  vedeexise orideat the lonfl\n",
      "raght jou while is increasive samen the usualled higheat one one four four feata\n",
      " ay inveffes and predexing life actor zero eaphery haleo and derindshazends and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.621597 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4200: 1.616953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4300: 1.622995 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4400: 1.608103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4500: 1.641006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4600: 1.622542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4700: 1.623310 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4800: 1.599884 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4900: 1.615592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5000: 1.612098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "j and in inducturied begordnoke are who undobac demanced any dunchands to it its\n",
      "x it in one four five one nine ponumeris was finds were not retresions bely of e\n",
      "jeherinum the chance produced roun eulvese and borrped be dde time momaged the e\n",
      "a depreadess portane werek excer dunges not rightor of the grewk gaung are mland\n",
      "girs one nine one one two loun ofless casel grawnsteneru been ba on and diet com\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.589302 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5200: 1.592732 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5300: 1.593861 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5400: 1.592553 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5500: 1.588479 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5600: 1.565002 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5700: 1.580540 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5800: 1.604539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5900: 1.581053 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6000: 1.581323 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "ly for the seach to by the a backns for ralleubbed of arted which of science one\n",
      "cal tradianuz most bardor tormverpigna prenambles dand one nine two nine nise of\n",
      "ring and and tects and and character heq litweighse from prisentant of march gen\n",
      "geor time selted gawe fire emprextbal not electrate unducely poet general hear a\n",
      "mpt their warnas was as combanion ty active estle warnes the times fighrses expa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.573342 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6200: 1.587148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6300: 1.580282 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6400: 1.573448 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.553007 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.598103 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.568599 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.574352 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.571054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 7000: 1.587891 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "wer of only mystlas of compegeral placked list and achigs refer tratation kime m\n",
      "k archho when philic died ethedest was simple nine vari in not development divis\n",
      "y to wair the used bowno viomer tendse town erding chesterial of leased debeard \n",
      "kn film a europe the john boing actcoragesae rathed communicationt hwar any milt\n",
      "rate keges and buring pyrnensip wut innascisment seats apsilection tober other s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(64, 100) * (100, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "(1, 100) * (100, 256) + (1, 64) * (64, 256) + (1, 256)\n"
     ]
    }
   ],
   "source": [
    "# 1. introduce embedding lookup on input\n",
    "\n",
    "num_nodes = 64\n",
    "embedding_size = 100 # embedding层的维度\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # 增加vocabulary_embeddings\n",
    "    # Input gate: input, previous output, and bias. \n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # 修改输入为100*64\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # 修改输入为100*64\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # 修改输入为100*64\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # 修改输入为100*64\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # 64*64\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes])) # 1*64\n",
    "\n",
    "    # 改动，拼接同类的tensor\n",
    "    tmp_x = tf.concat([ix, fx, cx, ox], 1) # 100*(64+64+64+64) = 100*256\n",
    "    tmp_m = tf.concat([im, fm, cm, om], 1) # 64*(64+64+64+64) = 64*256\n",
    "    tmp_b = tf.concat([ib, fb, cb, ob], 1) # 1*(64+64+64+64) = 1*256\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "#   # Definition of the cell computation.\n",
    "#     def lstm_cell(i, o, state):\n",
    "#         \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "#         Note that in this formulation, we omit the various connections between the\n",
    "#         previous state and the gates.\"\"\"\n",
    "#         input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#         forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#         update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#         state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#         output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "#         return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        print(i.shape, '*', tmp_x.shape, '+', o.shape, '*', tmp_m.shape, '+', tmp_b.shape)\n",
    "        smatmul = tf.matmul(i, tmp_x) + tf.matmul(o, tmp_m) + tmp_b \n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul, 4, 1)\n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "#         output, state = lstm_cell(i, output, state)\n",
    "#         outputs.append(output)\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1)) # 增加embedding代替input\n",
    "        output, state = lstm_cell(i_embed, output, state) # 对embedding计算lstm\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, \\\n",
    "        tf.argmax(sample_input, dimension=1)) # 增加sample的embedding层\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state) # 调用sample_input_embedding\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                            saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.308872 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.35\n",
      "================================================================================\n",
      "zs sb  ote t qdw  zprfe jphd vejodhr g e gqa y tkog geit s y tedxdp s acsbay ey \n",
      "e stfvwjjotsaqngu ihbnzelhprqsy  sdr r aaix ar de fon o eotc nefc wj fbiwwfksh n\n",
      "nalev evtiz hr ymxkp  pxelccez ada s   p aazm bhmv te kobpie hcj  ile crcnwceqo \n",
      "royrtn  ou gihvsthtre rhysv cbzliebtse oone acbrloqonx dc b ezyikn db aea x hpqc\n",
      "cth cuh i jojhfkcrtt cuh  fwo un todnar eo  c e ennj gqlgki  suwytptm ii sr e   \n",
      "================================================================================\n",
      "Validation set perplexity: 19.24\n",
      "Average loss at step 100: 2.301575 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.12\n",
      "Validation set perplexity: 8.71\n",
      "Average loss at step 200: 2.016815 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 300: 1.909146 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 400: 1.855073 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 500: 1.873136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 600: 1.809621 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 700: 1.786602 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 800: 1.778316 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 900: 1.775160 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1000: 1.713812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "king trad outcles mes that stadodion execututery sligage as viss more consites b\n",
      "her ox in ound zero one two nerm educes graphied the emophyly of smertrip is on \n",
      "cious a deceltively saswase with the dipsual resect or were formel hmong withelu\n",
      "ing occationst in is one a foud negestuated to one one nine to was ace abeted ba\n",
      "regelor of the del enesminer not three six free anyous is sletken at fance if pr\n",
      "================================================================================\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1100: 1.689785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1200: 1.724684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1300: 1.706505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1400: 1.682158 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1500: 1.676921 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.675843 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1700: 1.705868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1800: 1.668741 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1900: 1.670645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2000: 1.686493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "ves and kansia using coneterates estemclaa greek to refends dudinnted heating sa\n",
      "ceenists crokek years lends well frecialy the much but suchine are desence of ei\n",
      "ved to us tex are the stratel obseer of savil to antotocias brows the inons king\n",
      "ands creater protected the planga batsetided peresuteriis the live barplation te\n",
      "ve and sad to pleas shobs of the genevelly ejucherii relate of ania of allows oc\n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.678211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2200: 1.645432 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300: 1.654700 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2400: 1.659993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2500: 1.688416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.660742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2700: 1.676947 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2800: 1.635971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2900: 1.642310 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3000: 1.651629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "y in its of ampand is the talerimeral usions tadiman as lrdy sough knourt in the\n",
      "uses workmar enginemence willowern cmusity of in cern two five ladved of michang\n",
      "dios doing the lide of fonsass courts to nano his souskimage we poixing app dres\n",
      "x afternifia refilw the religebbern of aboutional of were the rong etecorical gr\n",
      "jects of rusts amerginal becumberse and rictism of islan use of enther of the ca\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3100: 1.650048 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3200: 1.647875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3300: 1.633156 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3400: 1.637333 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3500: 1.623823 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3600: 1.633961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3700: 1.629346 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3800: 1.629917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3900: 1.620421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4000: 1.623578 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "gain the ministrial viwids discomer of the refromes with secrodes hake that shot\n",
      "dical thesolnest that s interpervice order from the surrcather group the arms ho\n",
      "via chace carsor place that the mocirier in sumparbinationar villies salonar nim\n",
      "bansity they is cars naze by stromatis organifative to this whener xalled interb\n",
      "zating the wilednes was adition boan admitic form when techrous fullize insitume\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4100: 1.621179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4200: 1.610140 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4300: 1.591767 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4400: 1.627353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4500: 1.640956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4600: 1.638460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4700: 1.610461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4800: 1.591820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4900: 1.607061 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 5000: 1.635981 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "x for tackorlyzas cather operiom the tofas oliced u of the hoser wire bd a speci\n",
      "gante one and the move alovil case he atnale sociated with steam what deso sucpe\n",
      "more specha who schoess basic one fix compithy is english unisihn of necweul one\n",
      "oths mispalinatill as a sic type an eahas spectinuture the a foreiable n late of\n",
      "ones and consident lakracton car thin the for the first secoditives fooss the ju\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5100: 1.622526 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.605400 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5300: 1.568749 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.567083 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5500: 1.557656 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.582642 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5700: 1.541737 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.84\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.549431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5900: 1.570517 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.533436 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "ficatorwer idellensiation faction is accient revices the government in etemball \n",
      "jon hemocratics and c structlove to etechas of ther celly districties or wanre p\n",
      "gress the speciunstine later are no sonce deogy and becauside of nine two zero z\n",
      "cia and seckumbery this disagrate it inter four james chamar intered at ca only \n",
      "zataim the coonts beniellins after to do a starts that interbion large the playe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6100: 1.553185 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6200: 1.574985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6300: 1.585900 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.612918 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6500: 1.609443 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.578349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6700: 1.567038 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6800: 1.552492 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6900: 1.544843 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 7000: 1.555067 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "================================================================================\n",
      "diqaa and never forces of marically rictle no exterle placadaros barne are call \n",
      "zex interindents servensy its in chidel in sena parsons to use luxed on two zero\n",
      "hen a late an name modern backnally one nine zero two four zero zero writen witt\n",
      "z s pos of sfendentanisms and the bsimiskard mushilograte in the most with s bou\n",
      "ure islest and been union is one aganaby of religious the from the tame seven th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

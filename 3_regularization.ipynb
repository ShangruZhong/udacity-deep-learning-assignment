{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三章作业主要是学会处理神经网络的过拟合：\n",
    "- 引入正则化来缓解过拟合：用``tf.nn.l2_loss()``计算模型参数的L2范数，加入到loss中，具体如下：\n",
    "\n",
    "``` python\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + lmbda * (tf.nn.l2_loss(weights[0]) + tf.nn.l2_loss(weights[1])) # ==> add regularization\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "```\n",
    "- 用较大的minibatch样本数缓解过拟合：在用较小的minbatch样本训练会出现过拟合，即验证集达到100%准确率，但测试集准确率不高\n",
    "\n",
    "- 引入dropout来缓解过拟合：用``tf.nn.dropout(keep_prob)``在神经网络的训练中引入dropout，指定keep_prob，具体如下：\n",
    "\n",
    "``` python\n",
    "# tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)用法\n",
    "# 参数：训练样本x，keep_prob为训练时神经元节点保留概率\n",
    "\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "```\n",
    "\n",
    "- 用衰减学习率+多层神经网络改善模型性能：\n",
    "    - 学习率使用指数衰减，前期使用较大学习率来加快得到最优解，越往后epoch使用越小的学习率，避免靠近最优点时出现震荡，更好寻优；\n",
    "    - 引入多层神经网络，加两层ReLU，将上一层的ReLU输出再进行一次加权组合，W1(feature*hidden_nodes_1)，W2(hidden_nodes_1, hidden_nodes_2), W3(hidden_nodes_2, hidden_nodes_3), 和W4(hidden_nodes_3, num_labels)。\n",
    "\n",
    "``` python\n",
    "# tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)用法\n",
    "# 参数：初始值learning_rate, global_step, decay_steps, 衰减率decay_rate, staircase=False\n",
    "# decayed_learning_rate = learning_rate *\n",
    "#                        decay_rate ^ (global_step / decay_steps)\n",
    "# (staircase置True时，global_step/decay_steps为整除)\n",
    "# 每过一次decay_steps轮，衰减率乘一次decay_rate实现衰减\n",
    "\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, 500, 0.6, staircase=True) \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "```\n",
    "\n",
    "``` python\n",
    "def computation(dataset, weights, biases, is_dropout=False): \n",
    "    weight_sum_1 = tf.matmul(dataset, weights[0])+ biases[0]\n",
    "    hidden_layer_1 = tf.nn.relu(weight_sum_1)\n",
    "    if is_dropout:\n",
    "        hidden_layer_1 = tf.nn.dropout(hidden_layer_1, keep_prob=0.7)\n",
    "    weight_sum_2 = tf.matmul(hidden_layer_1, weights[1]) + biases[1]\n",
    "    hidden_layer_2 = tf.nn.relu(weight_sum_2)\n",
    "    if is_dropout:\n",
    "        hidden_layer_2 = tf.nn.dropout(hidden_layer_2, keep_prob=0.7)\n",
    "    weight_sum_3 = tf.matmul(hidden_layer_2, weights[2]) + biases[2]\n",
    "    hidden_layer_3 = tf.nn.relu(weight_sum_3)\n",
    "    if is_dropout:\n",
    "        hidden_layer_3 = tf.nn.dropout(hidden_layer_3, keep_prob=0.7)\n",
    "    outputs = tf.matmul(hidden_layer_3, weights[3]) + biases[3]\n",
    "    return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    # ==> define parameter of regularization\n",
    "    lmbda = tf.placeholder(tf.float32)\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + lmbda * tf.nn.l2_loss(weights)\n",
    "    # ==> add regularized item\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.376085\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 13.8%\n",
      "Minibatch loss at step 500: 2.657836\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1000: 1.627377\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1500: 1.153279\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2000: 0.969894\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2500: 0.691905\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 3000: 0.581506\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lmbda : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Impact of Lambda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized coefficients: 0.0001\n",
      "Regularized coefficients: 0.000125892541179\n",
      "Regularized coefficients: 0.000158489319246\n",
      "Regularized coefficients: 0.000199526231497\n",
      "Regularized coefficients: 0.000251188643151\n",
      "Regularized coefficients: 0.000316227766017\n",
      "Regularized coefficients: 0.000398107170553\n",
      "Regularized coefficients: 0.000501187233627\n",
      "Regularized coefficients: 0.00063095734448\n",
      "Regularized coefficients: 0.000794328234724\n",
      "Regularized coefficients: 0.001\n",
      "Regularized coefficients: 0.00125892541179\n",
      "Regularized coefficients: 0.00158489319246\n",
      "Regularized coefficients: 0.00199526231497\n",
      "Regularized coefficients: 0.00251188643151\n",
      "Regularized coefficients: 0.00316227766017\n",
      "Regularized coefficients: 0.00398107170553\n",
      "Regularized coefficients: 0.00501187233627\n",
      "Regularized coefficients: 0.0063095734448\n",
      "Regularized coefficients: 0.00794328234724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f87e87e80d0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCTP/HwI66RfyAlKhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f885808d950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "all_lmbda = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "all_accuracy = []\n",
    "for lmbda_val in all_lmbda:\n",
    "    print(\"Regularized coefficients: {}\".format(lmbda_val))\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lmbda : lmbda_val}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        all_accuracy.append(accuracy(test_prediction.eval(), test_labels))\n",
    "plt.plot(all_accuracy)\n",
    "plt.title(\"Accuracy over Regularizer Coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f87e8732cd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4nfGGOCjCV+Y4wJMv8fhi3P5uACvHEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f882b068e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_accuracy)\n",
    "plt.title(\"Accuracy over Regularizer Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (1-layer ReLU) with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 1024\n",
    "batch_size = 128\n",
    "\n",
    "def computation(dataset, weights, biases):\n",
    "    weight_sum = tf.add(tf.matmul(dataset, weights[0]), biases[0])\n",
    "    hidden_layer = tf.nn.relu(weight_sum)\n",
    "    logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "    return logits\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    lmbda = tf.placeholder(tf.float32) # ==> add placeholder\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = [tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes])), \n",
    "             tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "            ]\n",
    "    biases = [tf.Variable(tf.zeros([hidden_nodes])),\n",
    "            tf.Variable(tf.zeros([num_labels]))]\n",
    "\n",
    "    # Training computation.\n",
    "    logits = computation(tf_train_dataset, weights, biases)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + lmbda * (tf.nn.l2_loss(weights[0]) + tf.nn.l2_loss(weights[1])) # ==> add regularization\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(computation(tf_valid_dataset, weights, biases))\n",
    "    test_prediction = tf.nn.softmax(computation(tf_test_dataset, weights, biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 617.020447\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 34.7%\n",
      "Minibatch loss at step 500: 206.956284\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 1000: 113.745140\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 68.286926\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2000: 41.318298\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2500: 25.188065\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000: 15.299156\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.3%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lmbda : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.644571\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 13.5%\n",
      "Minibatch loss at step 500: 1.961034\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 1000: 1.173835\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 1500: 0.738649\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 2000: 0.498035\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 2500: 0.356002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 3000: 0.271700\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Test accuracy: 84.4%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = ((step % 10) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lmbda : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论：**计算minibatch的offet时，修改为``offset = ((step % 10) * batch_size) % (train_labels.shape[0] - batch_size)``，限制反复使用相同的10个batch数据（每个batch有128个样本），每次epoch使用一个batch。可以看到在2500次迭代后，minibatch的数据可以达到100%准确率，但在测试集上只有84.4%，远不如之前的93.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 1024\n",
    "batch_size = 128\n",
    "\n",
    "def computation(dataset, weights, biases, is_dropout=False, keep_prob=0.5):\n",
    "    weight_sum = tf.add(tf.matmul(dataset, weights[0]), biases[0])\n",
    "    hidden_layer = tf.nn.relu(weight_sum)\n",
    "    if is_dropout: # ==> add dropout\n",
    "        hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "    logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "    return logits\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    lmbda = tf.placeholder(tf.float32) # ==> add placeholder\n",
    "    keep_prob = tf.placeholder(tf.float32) # ==> add placeholder\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = [tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes])), \n",
    "             tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))]\n",
    "    biases = [tf.Variable(tf.zeros([hidden_nodes])),\n",
    "            tf.Variable(tf.zeros([num_labels]))]\n",
    "\n",
    "    # Training computation.\n",
    "    logits = computation(tf_train_dataset, weights, biases, is_dropout=True, keep_prob=keep_prob)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + lmbda * (tf.nn.l2_loss(weights[0]) + tf.nn.l2_loss(weights[1])) # ==> add regularization\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(computation(tf_valid_dataset, weights, biases))\n",
    "    test_prediction = tf.nn.softmax(computation(tf_test_dataset, weights, biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 746.116699\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 32.7%\n",
      "Minibatch loss at step 500: 200.717834\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1000: 114.825737\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1500: 68.643456\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2000: 41.425804\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2500: 25.150757\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 15.244875\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.2%\n",
      "Test accuracy: 92.7%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lmbda : 1e-3, keep_prob : 0.7}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论：**只在训练中加入dropout操作，验证和测试时不加入。随机丢弃一部分神经元，以一定概率使得神经元激活函数不参与。貌似只能解决小minibatch的过拟合问题，对于大minibatch时，加入dropout效果迭代3000次得到（92.7%）并不如不加的（93.3%）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes_1 = 1024\n",
    "hidden_nodes_2 = 500\n",
    "hidden_nodes_3 = 100\n",
    "batch_size = 128\n",
    "\n",
    "def computation(dataset, weights, biases, is_dropout=False): \n",
    "    weight_sum_1 = tf.matmul(dataset, weights[0])+ biases[0]\n",
    "    hidden_layer_1 = tf.nn.relu(weight_sum_1)\n",
    "    if is_dropout:\n",
    "        hidden_layer_1 = tf.nn.dropout(hidden_layer_1, keep_prob=0.7)\n",
    "    weight_sum_2 = tf.matmul(hidden_layer_1, weights[1]) + biases[1]\n",
    "    hidden_layer_2 = tf.nn.relu(weight_sum_2)\n",
    "    if is_dropout:\n",
    "        hidden_layer_2 = tf.nn.dropout(hidden_layer_2, keep_prob=0.7)\n",
    "    weight_sum_3 = tf.matmul(hidden_layer_2, weights[2]) + biases[2]\n",
    "    hidden_layer_3 = tf.nn.relu(weight_sum_3)\n",
    "    if is_dropout:\n",
    "        hidden_layer_3 = tf.nn.dropout(hidden_layer_3, keep_prob=0.7)\n",
    "    outputs = tf.matmul(hidden_layer_3, weights[3]) + biases[3]\n",
    "    return outputs\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    lmbda = tf.placeholder(tf.float32) # ==> add placeholder\n",
    "#     keep_prob = tf.placeholder(tf.float32) # ==> add placeholder\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    global_step = tf.Variable(0) # ==> Add for learning rate decay\n",
    "    \n",
    "    # Variables.\n",
    "    weights = [tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_1],\n",
    "                                stddev=np.sqrt(2.0 / (image_size * image_size)))), \n",
    "            tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2],\n",
    "                                stddev=np.sqrt(2.0 / hidden_nodes_1))),\n",
    "            tf.Variable(tf.truncated_normal([hidden_nodes_2, hidden_nodes_3],\n",
    "                                stddev=np.sqrt(2.0 / hidden_nodes_2))),\n",
    "            tf.Variable(tf.truncated_normal([hidden_nodes_3, num_labels],\n",
    "                                stddev=np.sqrt(2.0 / hidden_nodes_3)))\n",
    "               \n",
    "              ]\n",
    "\n",
    "    biases = [tf.Variable(tf.zeros([hidden_nodes_1])),\n",
    "            tf.Variable(tf.zeros([hidden_nodes_2])),\n",
    "            tf.Variable(tf.zeros([hidden_nodes_3])),\n",
    "            tf.Variable(tf.zeros([num_labels]))\n",
    "             ]\n",
    "\n",
    "    # Training computation.\n",
    "    logits = computation(tf_train_dataset, weights, biases, is_dropout=True) \n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "        + lmbda * (tf.nn.l2_loss(weights[0]) \\\n",
    "        + tf.nn.l2_loss(weights[1]) \\\n",
    "        + tf.nn.l2_loss(weights[2]) \\\n",
    "        + tf.nn.l2_loss(weights[3])) # ==> add regularization\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 500, 0.9)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(computation(tf_valid_dataset, weights, biases))\n",
    "    test_prediction = tf.nn.softmax(computation(tf_test_dataset, weights, biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.903420\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 31.9%\n",
      "Minibatch loss at step 500: 1.387359\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1000: 0.973619\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1500: 0.967071\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2000: 0.897126\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2500: 0.560037\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3000: 0.477757\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 3500: 0.596783\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 4000: 0.578859\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4500: 0.515870\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 5000: 0.603967\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 5500: 0.420070\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6000: 0.486005\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 6500: 0.555979\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 7000: 0.377530\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 7500: 0.470899\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 8000: 0.398080\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Test accuracy: 95.2%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "num_steps = 8001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lmbda : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
